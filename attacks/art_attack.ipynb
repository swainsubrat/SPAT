{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mThe script demonstrates a simple example of using ART with PyTorch. The example train a small model on the MNIST dataset\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mand creates adversarial examples using the Fast Gradient Sign Method. Here we use the ART classifier to train the model,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mit would also be possible to provide a pretrained model to the ART classifier.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mThe parameters are chosen for reduced computational requirements of the script and not optimised for accuracy.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mart\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevasion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastGradientMethod, DeepFool\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mart\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mestimators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassification\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PyTorchClassifier\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The script demonstrates a simple example of using ART with PyTorch. The example train a small model on the MNIST dataset\n",
    "and creates adversarial examples using the Fast Gradient Sign Method. Here we use the ART classifier to train the model,\n",
    "it would also be possible to provide a pretrained model to the ART classifier.\n",
    "The parameters are chosen for reduced computational requirements of the script and not optimised for accuracy.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "from art.attacks.evasion import FastGradientMethod, DeepFool\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from dataloader import load_mnist\n",
    "from typing import Tuple, Callable\n",
    "from models.classifier import (MNISTClassifier, CIFAR10Classifier,\n",
    "                                CelebAClassifier)\n",
    "from models.autoencoder import (ANNAutoencoder, BaseAutoEncoder,\n",
    "                                CIFAR10Autoencoder, CelebAAutoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Step 0: load the config\n",
    "with open(\"./configs/mnist.yml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "ROOT             = config[\"paths\"][\"root\"]\n",
    "AUTOENCODER_PATH = config[\"paths\"][\"autoencoder-path\"]\n",
    "CLASSIFIER_PATH  = config[\"paths\"][\"classifier-path\"]\n",
    "BOUNDS           = (config[\"specs\"][\"bounds\"][0], config[\"specs\"][\"bounds\"][1])\n",
    "PLOT             = config[\"specs\"][\"plot\"]\n",
    "BATCH_SIZE       = config[\"specs\"][\"batch_size\"]\n",
    "RESHAPE          = (config[\"specs\"][\"reshape\"][0], config[\"specs\"][\"reshape\"][1])\n",
    "LOAD_FUNCTION    = load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 10/10 [00:00<00:00, 16.82it/s]\n",
      "Accuracy on benign test examples: 98.00999760627747%\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the model(trained)\n",
    "mnist_classifier  = MNISTClassifier.load_from_checkpoint(CLASSIFIER_PATH).to(device)\n",
    "mnist_classifier.eval()\n",
    "\n",
    "# Step 2: Load the dataset\n",
    "_, _, test_dataloader = LOAD_FUNCTION(batch_size=1000, root=ROOT)\n",
    "x_test, y_test = next(iter(test_dataloader))\n",
    "x_test_np, y_test_np = x_test.numpy(), y_test.numpy()\n",
    "x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "\n",
    "# Step 2a: Predict with benign samples\n",
    "trainer = pl.Trainer(max_epochs=10, gpus=1, default_root_dir=\"..\")\n",
    "p = trainer.test(mnist_classifier, dataloaders=test_dataloader, verbose=False)\n",
    "print(\"Accuracy on benign test examples: {}%\".format(p[0][\"test_acc\"] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ## Making Hybrid Models\n",
    "mnist_autoencoder = ANNAutoencoder.load_from_checkpoint(AUTOENCODER_PATH).to(device)\n",
    "mnist_autoencoder.eval()\n",
    "\n",
    "hybrid_classifier = nn.Sequential(\n",
    "        mnist_autoencoder.decoder,\n",
    "        mnist_classifier.model\n",
    "    )\n",
    "z_test = mnist_autoencoder.get_z(x_test)\n",
    "z_test_np = z_test.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define the loss function and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(mnist_classifier.parameters(), lr=0.01)\n",
    "\n",
    "# Step 4: Create the ART classifier\n",
    "classifier = PyTorchClassifier(\n",
    "    model=mnist_classifier,\n",
    "    # clip_values=(min_pixel_value, max_pixel_value),\n",
    "    loss=criterion,\n",
    "    # optimizer=optimizer,\n",
    "    input_shape=(1, 784),\n",
    "    nb_classes=10,\n",
    ")\n",
    "\n",
    "# # Step 4a: Train the ART classifier\n",
    "# # classifier.fit(x_train, y_train, batch_size=64, nb_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on benign test examples: 97.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeepFool: 100%|██████████| 1000/1000 [01:00<00:00, 16.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on adversarial test examples: 1.7999999999999998%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Evaluate the ART classifier on benign test examples\n",
    "predictions = classifier.predict(x_test_np)\n",
    "accuracy = np.sum(np.argmax(predictions, -1) == y_test_np)/ len(y_test_np)\n",
    "print(\"Accuracy on benign test examples: {}%\".format(accuracy * 100))\n",
    "\n",
    "# Step 6: Generate adversarial test examples\n",
    "attack = DeepFool(classifier)\n",
    "x_test_adv = attack.generate(x=x_test_np)\n",
    "\n",
    "# Step 7: Evaluate the ART classifier on adversarial test examples\n",
    "predictions = classifier.predict(x_test_adv)\n",
    "accuracy = np.sum(np.argmax(predictions, axis=-1) == y_test_np) / len(y_test_np)\n",
    "print(\"Accuracy on adversarial test examples: {}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define the loss function and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(mnist_classifier.parameters(), lr=0.01)\n",
    "\n",
    "# Step 4: Create the ART classifier\n",
    "classifier = PyTorchClassifier(\n",
    "    model=hybrid_classifier,\n",
    "    # clip_values=(min_pixel_value, max_pixel_value),\n",
    "    loss=criterion,\n",
    "    # optimizer=optimizer,\n",
    "    input_shape=(1, 128),\n",
    "    nb_classes=10,\n",
    ")\n",
    "\n",
    "# # Step 4a: Train the ART classifier\n",
    "# # classifier.fit(x_train, y_train, batch_size=64, nb_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on benign test examples: 96.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeepFool: 100%|██████████| 1000/1000 [00:59<00:00, 16.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on adversarial test examples: 2.8000000000000003%\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True False  True  True  True\n",
      "  True  True  True  True  True  True  True False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True False  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True False  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True False  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True False  True False False  True  True  True  True  True  True  True\n",
      " False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True False  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True False  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False  True  True  True  True  True  True False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True False  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True False  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True False  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False  True  True  True  True  True  True  True  True  True  True  True\n",
      " False  True  True  True  True  True  True  True  True  True  True False\n",
      "  True  True  True  True  True  True  True  True False False  True  True\n",
      "  True  True  True  True  True False  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Evaluate the ART classifier on benign test examples\n",
    "predictions = classifier.predict(z_test_np)\n",
    "accuracy = np.sum(np.argmax(predictions, -1) == y_test_np)/ len(y_test_np)\n",
    "print(\"Accuracy on benign test examples: {}%\".format(accuracy * 100))\n",
    "\n",
    "# Step 6: Generate adversarial test examples\n",
    "attack = DeepFool(classifier)\n",
    "z_test_adv_np = attack.generate(x=z_test_np)\n",
    "\n",
    "# Step 7: Evaluate the ART classifier on adversarial test examples\n",
    "predictions = classifier.predict(z_test_adv_np)\n",
    "accuracy = np.sum(np.argmax(predictions, axis=-1) == y_test_np) / len(y_test_np)\n",
    "print(\"Accuracy on adversarial test examples: {}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title ## Visualise the results\n",
    "z_test_adv = torch.Tensor(z_test_adv_np).to(device)\n",
    "x_test_hat = mnist_autoencoder.get_x_hat(z_test_adv)\n",
    "\n",
    "plt.imshow(x_test_hat[1].detach().cpu().reshape(28, 28))\n",
    "torch.argmax(mnist_classifier(x_test_hat[-2])).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import save\n",
    "save(path=\"./objects/adv_images_1000_mnist.pkl\", params=x_test_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras.models import load_model\n",
    "\n",
    "from art import config\n",
    "from art.utils import load_dataset, get_file\n",
    "from art.estimators.classification import KerasClassifier\n",
    "from art.attacks.evasion import FastGradientMethod\n",
    "from art.attacks.evasion import BasicIterativeMethod\n",
    "from art.defences.trainer import AdversarialTrainer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = get_file('mnist_cnn_robust.h5', extract=False, path=config.ART_DATA_PATH,\n",
    "                url='https://www.dropbox.com/s/yutsncaniiy5uy8/mnist_cnn_robust.h5?dl=1')\n",
    "robust_classifier_model = load_model(path)\n",
    "(x_train, y_train), (x_test, y_test), min_, max_ = load_dataset('mnist')\n",
    "robust_classifier = KerasClassifier(clip_values=(min_, max_), model=robust_classifier_model, use_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
