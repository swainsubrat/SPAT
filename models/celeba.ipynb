{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoder import CelebAAutoencoder, CelebAAutoencoderNew\n",
    "\n",
    "import os\n",
    "import torchsummary\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from dataloader import load_celeba\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5, 6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "MisconfigurationException",
     "evalue": "`Trainer(strategy='ddp_spawn')` or `Trainer(accelerator='ddp_spawn')` is not compatible with an interactive environment. Run your code as a script, or choose one of the compatible strategies: Trainer(strategy=None|dp|tpu_spawn). In case you are spawning processes yourself, make sure to include the Trainer creation inside the worker function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m/home/sweta/src/others/Semantic-Preserving-Adversarial-Attack/models/celeba.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Buq-cvl/home/sweta/src/others/Semantic-Preserving-Adversarial-Attack/models/celeba.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m CelebAAutoencoder(lr\u001b[39m=\u001b[39m\u001b[39m1e-5\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Buq-cvl/home/sweta/src/others/Semantic-Preserving-Adversarial-Attack/models/celeba.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# torchsummary.summary(model, (3, 128, 128))\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Buq-cvl/home/sweta/src/others/Semantic-Preserving-Adversarial-Attack/models/celeba.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39;49mTrainer(max_epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, gpus\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, default_root_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m..\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Buq-cvl/home/sweta/src/others/Semantic-Preserving-Adversarial-Attack/models/celeba.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# # trainer.fit(model, train_dataloader)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Buq-cvl/home/sweta/src/others/Semantic-Preserving-Adversarial-Attack/models/celeba.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m train_dataloader, test_dataloader \u001b[39m=\u001b[39m load_celeba(batch_size\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n",
      "File \u001b[0;32m~/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/utilities/argparse.py:339\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/utilities/argparse.py?line=335'>336</a>\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mlist\u001b[39m(env_variables\u001b[39m.\u001b[39mitems()) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(kwargs\u001b[39m.\u001b[39mitems()))\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/utilities/argparse.py?line=337'>338</a>\u001b[0m \u001b[39m# all args were already moved to kwargs\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/utilities/argparse.py?line=338'>339</a>\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:483\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, logger, checkpoint_callback, enable_checkpointing, callbacks, default_root_dir, gradient_clip_val, gradient_clip_algorithm, process_position, num_nodes, num_processes, devices, gpus, auto_select_gpus, tpu_cores, ipus, log_gpu_memory, progress_bar_refresh_rate, enable_progress_bar, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, val_check_interval, flush_logs_every_n_steps, log_every_n_steps, accelerator, strategy, sync_batchnorm, precision, enable_model_summary, weights_summary, weights_save_path, num_sanity_val_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_n_epochs, auto_lr_find, replace_sampler_ddp, detect_anomaly, auto_scale_batch_size, prepare_data_per_node, plugins, amp_backend, amp_level, move_metrics_to_cpu, multiple_trainloader_mode, stochastic_weight_avg, terminate_on_nan)\u001b[0m\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=479'>480</a>\u001b[0m \u001b[39m# init connectors\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=480'>481</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector \u001b[39m=\u001b[39m DataConnector(\u001b[39mself\u001b[39m, multiple_trainloader_mode)\n\u001b[0;32m--> <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=482'>483</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_connector \u001b[39m=\u001b[39m AcceleratorConnector(\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=483'>484</a>\u001b[0m     num_processes\u001b[39m=\u001b[39;49mnum_processes,\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=484'>485</a>\u001b[0m     devices\u001b[39m=\u001b[39;49mdevices,\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=485'>486</a>\u001b[0m     tpu_cores\u001b[39m=\u001b[39;49mtpu_cores,\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=486'>487</a>\u001b[0m     ipus\u001b[39m=\u001b[39;49mipus,\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=487'>488</a>\u001b[0m     accelerator\u001b[39m=\u001b[39;49maccelerator,\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=488'>489</a>\u001b[0m     strategy\u001b[39m=\u001b[39;49mstrategy,\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=489'>490</a>\u001b[0m     gpus\u001b[39m=\u001b[39;49mgpus,\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=490'>491</a>\u001b[0m     num_nodes\u001b[39m=\u001b[39;49mnum_nodes,\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=491'>492</a>\u001b[0m     sync_batchnorm\u001b[39m=\u001b[39;49msync_batchnorm,\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=492'>493</a>\u001b[0m     benchmark\u001b[39m=\u001b[39;49mbenchmark,\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=493'>494</a>\u001b[0m     replace_sampler_ddp\u001b[39m=\u001b[39;49mreplace_sampler_ddp,\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=494'>495</a>\u001b[0m     deterministic\u001b[39m=\u001b[39;49mdeterministic,\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=495'>496</a>\u001b[0m     auto_select_gpus\u001b[39m=\u001b[39;49mauto_select_gpus,\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=496'>497</a>\u001b[0m     precision\u001b[39m=\u001b[39;49mprecision,\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=497'>498</a>\u001b[0m     amp_type\u001b[39m=\u001b[39;49mamp_backend,\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=498'>499</a>\u001b[0m     amp_level\u001b[39m=\u001b[39;49mamp_level,\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=499'>500</a>\u001b[0m     plugins\u001b[39m=\u001b[39;49mplugins,\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=500'>501</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=501'>502</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_logger_connector \u001b[39m=\u001b[39m LoggerConnector(\u001b[39mself\u001b[39m, log_gpu_memory)\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=502'>503</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_connector \u001b[39m=\u001b[39m CallbackConnector(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:212\u001b[0m, in \u001b[0;36mAcceleratorConnector.__init__\u001b[0;34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, amp_type, amp_level, sync_batchnorm, benchmark, replace_sampler_ddp, deterministic, auto_select_gpus, num_processes, tpu_cores, ipus, gpus)\u001b[0m\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=208'>209</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_and_init_precision()\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=210'>211</a>\u001b[0m \u001b[39m# 6. Instantiate Strategy - Part 2\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=211'>212</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lazy_init_strategy()\n",
      "File \u001b[0;32m~/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:783\u001b[0m, in \u001b[0;36mAcceleratorConnector._lazy_init_strategy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=779'>780</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m \u001b[39mimport\u001b[39;00m _IS_INTERACTIVE\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=781'>782</a>\u001b[0m \u001b[39mif\u001b[39;00m _IS_INTERACTIVE \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mis_interactive_compatible:\n\u001b[0;32m--> <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=782'>783</a>\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=783'>784</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`Trainer(strategy=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mstrategy_name\u001b[39m!r}\u001b[39;00m\u001b[39m)` or\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=784'>785</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m `Trainer(accelerator=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mstrategy_name\u001b[39m!r}\u001b[39;00m\u001b[39m)` is not compatible with an interactive\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=785'>786</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m environment. Run your code as a script, or choose one of the compatible strategies:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=786'>787</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Trainer(strategy=None|\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m|\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(_StrategyType\u001b[39m.\u001b[39minteractive_compatible_types())\u001b[39m}\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=787'>788</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m In case you are spawning processes yourself, make sure to include the Trainer\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=788'>789</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m creation inside the worker function.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=789'>790</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=791'>792</a>\u001b[0m \u001b[39m# TODO: should be moved to _check_strategy_and_fallback().\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=792'>793</a>\u001b[0m \u001b[39m# Current test check precision first, so keep this check here to meet error order\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=793'>794</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator, TPUAccelerator) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=794'>795</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy, (SingleTPUStrategy, TPUSpawnStrategy)\n\u001b[1;32m    <a href='file:///home/sweta/scratch/miniconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py?line=795'>796</a>\u001b[0m ):\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: `Trainer(strategy='ddp_spawn')` or `Trainer(accelerator='ddp_spawn')` is not compatible with an interactive environment. Run your code as a script, or choose one of the compatible strategies: Trainer(strategy=None|dp|tpu_spawn). In case you are spawning processes yourself, make sure to include the Trainer creation inside the worker function."
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "train_dataloader, valid_dataloader, test_dataloader = load_celeba(batch_size=64)\n",
    "\n",
    "model = CelebAAutoencoder(lr=1e-5)\n",
    "# torchsummary.summary(model, (3, 128, 128))\n",
    "trainer = pl.Trainer(max_epochs=50, gpus=2, default_root_dir=\"..\")\n",
    "# # trainer.fit(model, train_dataloader)\n",
    "\n",
    "train_dataloader, test_dataloader = load_celeba(batch_size=4)\n",
    "input_imgs, _ = next(iter(test_dataloader))\n",
    "model = CelebAAutoencoder.load_from_checkpoint(\"../lightning_logs/celeba_ae_mse/checkpoints/epoch=49-step=31700.ckpt\")\n",
    "model.eval()\n",
    "reconst_imgs = model(input_imgs)\n",
    "\n",
    "input_imgs   = input_imgs.reshape(3, 128, 128).detach()\n",
    "reconst_imgs = reconst_imgs.reshape(3, 128, 128).detach()\n",
    "fig, axis = plt.subplots(1,2)\n",
    "\n",
    "axis[0].imshow(np.transpose(input_imgs, (1, 2, 0)))\n",
    "axis[1].imshow(np.transpose(reconst_imgs, (1, 2, 0)))\n",
    "\n",
    "# plt.savefig(f\"../img/celeba_reconstruction1.png\", dpi=1000)\n",
    "plt.show()\n",
    "# visualize_cifar_reconstructions(input_imgs, reconst_imgs, file_name=\"celeba_ae_8mse_2bce_recons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
