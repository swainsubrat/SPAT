{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture installation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import yaml\n",
    "from art.attacks.evasion import DeepFool, FastGradientMethod\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "\n",
    "from typing import Callable, Tuple, Dict\n",
    "from pathlib import Path\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "\n",
    "from dataloader import load_mnist\n",
    "from models.autoencoder import (ANNAutoencoder, BaseAutoEncoder,\n",
    "                                CelebAAutoencoder, CIFAR10Autoencoder)\n",
    "from models.classifier import (CelebAClassifier, CIFAR10Classifier,\n",
    "                                MNISTClassifier)\n",
    "\n",
    "from attacks import ATTACK_MAPPINGS\n",
    "from attacks.art_attack import execute_attack, get_models, get_xyz, hybridize\n",
    "from attacks.plot_attack import plot_adversarial_images, plot_robust_accuracy\n",
    "from dataloader import DATALOADER_MAPPINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    batch_size = 1\n",
    "    dataset_len = 1000\n",
    "    device  = \"cuda\"\n",
    "    model_name = \"cifar10_cnn_1\"\n",
    "    ae_name = \"cnn_256\"\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on the dataset: cifar10!!!!!\n",
      "Loaded classifier and autoencoder models in eval mode!!!!!\n",
      "Files already downloaded and verified\n",
      "Loaded dataloader cifar101000!!!!!\n"
     ]
    }
   ],
   "source": [
    "dataset_name = args.model_name.split(\"_\")[0]\n",
    "print(f\"Working on the dataset: {dataset_name}!!!!!\")\n",
    "\n",
    "with open(f\"./configs/{dataset_name}.yml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "classifier_model, autoencoder_model, config = get_models(args)\n",
    "print(f\"Loaded classifier and autoencoder models in eval mode!!!!!\")\n",
    "train_dataloader = DATALOADER_MAPPINGS[config[\"dataset_name\"]](batch_size=args.batch_size)\n",
    "print(f\"Loaded dataloader {config['dataset_name']}!!!!!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_adversarial_images(images, pgd_adv_images, spgd_adv_images, reshape_size=(28, 28)):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(1,5,1, xticks=[], yticks=[])\n",
    "    plt.title(\"Original\")\n",
    "    plt.imshow(images[0].cpu().detach().reshape(reshape_size).permute(1, 2, 0))\n",
    "\n",
    "    plt.subplot(1,5,2, xticks=[], yticks=[])\n",
    "    plt.title(\"PGD\")\n",
    "    plt.imshow(pgd_adv_images[0].cpu().detach().reshape(reshape_size).permute(1, 2, 0))\n",
    "\n",
    "    pgd_noise = pgd_adv_images[0] - images[0]\n",
    "    plt.subplot(1,5,3, xticks=[], yticks=[])\n",
    "    plt.title(\"PGD Noise\")\n",
    "    plt.imshow(pgd_noise.cpu().detach().reshape(reshape_size).permute(1, 2, 0))\n",
    "\n",
    "    plt.subplot(1,5,4, xticks=[], yticks=[])\n",
    "    plt.title(\"SemanticPGD\")\n",
    "    plt.imshow(spgd_adv_images[0].cpu().detach().reshape(reshape_size).permute(1, 2, 0))\n",
    "\n",
    "    spgd_noise = spgd_adv_images[0] - images[0]\n",
    "    plt.subplot(1,5,5, xticks=[], yticks=[])\n",
    "    plt.title(\"SemanticPGD Noise\")\n",
    "    plt.imshow(spgd_noise.cpu().detach().reshape(reshape_size).permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_attack(model, images, labels, eps=0.3, alpha=2/255, iters=40) :\n",
    "    images = images.to(args.device)\n",
    "    labels = labels.to(args.device)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "    ori_images = images.data\n",
    "        \n",
    "    for i in range(iters) :  \n",
    "        images.requires_grad = True\n",
    "        outputs = model(images)\n",
    "\n",
    "        model.zero_grad()\n",
    "        cost = loss(outputs, labels).to(args.device)\n",
    "        cost.backward()\n",
    "\n",
    "        # Perturb original image\n",
    "        adv_images = images + alpha*images.grad.sign()\n",
    "\n",
    "        eta = torch.clamp(adv_images - ori_images, min=-eps, max=eps)\n",
    "        images = torch.clamp(ori_images + eta, min=0, max=1).detach_()\n",
    "            \n",
    "    return images\n",
    "\n",
    "def semantic_pgd_attack(model, ae_model, images, labels, eps=0.4, alpha=8/255, s_alpha=0.007, iters=100, batch_size=1) :\n",
    "    images = images.to(args.device)\n",
    "    labels = labels.to(args.device)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "    adv_images = images.clone().detach()\n",
    "    original_inputs_numpy = images.clone().cpu().detach().numpy()\n",
    "        \n",
    "    for i in range(iters) :\n",
    "        adv_images.requires_grad = True\n",
    "        # adv_images = adv_images.to(args.device)\n",
    "        \n",
    "        # Classifier part\n",
    "        outputs = model(adv_images)\n",
    "\n",
    "        model.zero_grad()\n",
    "        cost = loss(outputs, labels).to(args.device)\n",
    "        cost.backward()\n",
    "\n",
    "        # # Perturb original image\n",
    "        # adv_images = images + alpha*images.grad.sign()\n",
    "\n",
    "        # # Adjust semantics\n",
    "        # adv_images = adv_images.detach_()\n",
    "        # adv_images.requires_grad = True\n",
    "        # recon_images, _ = ae_model(adv_images)\n",
    "        # ae_model.zero_grad()\n",
    "\n",
    "        # Calculate grad w.r.t classifier\n",
    "        grad_classifier = adv_images.grad.cpu().detach()\n",
    "\n",
    "        # Autoencoder part\n",
    "        adv_images.grad = None\n",
    "        recon_images, _ = ae_model(adv_images)\n",
    "        ae_model.zero_grad()\n",
    "\n",
    "        mse_loss = F.mse_loss(adv_images, recon_images).to(args.device)\n",
    "        mse_loss.backward()\n",
    "\n",
    "        # Calculate grad w.r.t autoencoder\n",
    "        grad_autoencoder = adv_images.grad.cpu().detach()\n",
    "\n",
    "        # Check if the attack is successful\n",
    "        has_attack_succeeded = (outputs.cpu().detach().numpy().argmax(1)!=labels.cpu().numpy())\n",
    "\n",
    "        # Calculate the projection of perceptual grad on classifier\n",
    "        # and grad_autoencoder = grad_autoencoder - (projection of gradient_autoencoder onto gradient_classifier)\n",
    "        grad_autoencoder_proj = grad_autoencoder - torch.bmm((torch.bmm(grad_autoencoder.view(batch_size, 1, -1), \n",
    "                                grad_classifier.view(batch_size, -1, 1)))/(1e-20+torch.bmm(grad_classifier.view(batch_size, 1, -1),\n",
    "                                grad_classifier.view(batch_size, -1, 1))).view(-1, 1, 1),\n",
    "                                grad_classifier.view(batch_size, 1, -1)).view(grad_autoencoder.shape)\n",
    "\n",
    "        # Calculate the projection of classifier grad on autoencoder\n",
    "        # and grad_classifier = grad_classifier - (projection of gradient_classifier onto gradient_autoencoder)\n",
    "        grad_classifier_proj = grad_classifier - torch.bmm((torch.bmm(grad_classifier.view(batch_size, 1, -1),\n",
    "                               grad_autoencoder.view(batch_size, -1, 1)))/(1e-20+torch.bmm(grad_autoencoder.view(batch_size, 1, -1),\n",
    "                               grad_autoencoder.view(batch_size, -1, 1))).view(-1, 1, 1),\n",
    "                               grad_autoencoder.view(batch_size, 1, -1)).view(grad_classifier.shape)\n",
    "\n",
    "        # Combine grads (Selective Gradient Descent)\n",
    "        # grad = grad_classifier * (1-has_attack_succeeded) - grad_autoencoder * has_attack_succeeded\n",
    "        grad = grad_classifier_proj * (1-has_attack_succeeded) - (grad_autoencoder_proj) * has_attack_succeeded\n",
    "\n",
    "        # Add semantic perturbation\n",
    "        # adv_examples = adv_images - s_alpha * sign_data_grad\n",
    "\n",
    "        # Apply combined perturbation\n",
    "        sign_grad = torch.sign(grad).to(torch.float32).to(args.device)\n",
    "        adv_images = adv_images.detach() + alpha * sign_grad\n",
    "\n",
    "        eta = torch.clamp(adv_images - torch.tensor(original_inputs_numpy).to(args.device), min=-eps, max=eps)\n",
    "        adv_images = torch.clamp(torch.tensor(original_inputs_numpy).to(args.device) + eta, min=0, max=1).detach()\n",
    "            \n",
    "    return adv_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of validation dataloader: 1000\n",
      "0 iterations done!!\n",
      "10 iterations done!!\n",
      "20 iterations done!!\n",
      "30 iterations done!!\n",
      "40 iterations done!!\n",
      "50 iterations done!!\n",
      "60 iterations done!!\n",
      "70 iterations done!!\n",
      "80 iterations done!!\n",
      "90 iterations done!!\n",
      "100 iterations done!!\n",
      "110 iterations done!!\n",
      "120 iterations done!!\n",
      "130 iterations done!!\n",
      "140 iterations done!!\n",
      "150 iterations done!!\n",
      "160 iterations done!!\n",
      "170 iterations done!!\n",
      "180 iterations done!!\n",
      "190 iterations done!!\n",
      "200 iterations done!!\n",
      "210 iterations done!!\n",
      "220 iterations done!!\n",
      "230 iterations done!!\n",
      "240 iterations done!!\n",
      "250 iterations done!!\n",
      "260 iterations done!!\n",
      "270 iterations done!!\n",
      "280 iterations done!!\n",
      "290 iterations done!!\n",
      "300 iterations done!!\n",
      "310 iterations done!!\n",
      "320 iterations done!!\n",
      "330 iterations done!!\n",
      "340 iterations done!!\n",
      "350 iterations done!!\n",
      "360 iterations done!!\n",
      "370 iterations done!!\n",
      "380 iterations done!!\n",
      "390 iterations done!!\n",
      "400 iterations done!!\n",
      "410 iterations done!!\n",
      "420 iterations done!!\n",
      "430 iterations done!!\n",
      "440 iterations done!!\n",
      "450 iterations done!!\n",
      "460 iterations done!!\n",
      "470 iterations done!!\n",
      "480 iterations done!!\n",
      "490 iterations done!!\n",
      "500 iterations done!!\n",
      "510 iterations done!!\n",
      "520 iterations done!!\n",
      "530 iterations done!!\n",
      "540 iterations done!!\n",
      "550 iterations done!!\n",
      "560 iterations done!!\n",
      "570 iterations done!!\n",
      "580 iterations done!!\n",
      "590 iterations done!!\n",
      "600 iterations done!!\n",
      "610 iterations done!!\n",
      "620 iterations done!!\n",
      "630 iterations done!!\n",
      "640 iterations done!!\n",
      "650 iterations done!!\n",
      "660 iterations done!!\n",
      "670 iterations done!!\n",
      "680 iterations done!!\n",
      "690 iterations done!!\n",
      "700 iterations done!!\n",
      "710 iterations done!!\n",
      "720 iterations done!!\n",
      "730 iterations done!!\n",
      "740 iterations done!!\n",
      "750 iterations done!!\n",
      "760 iterations done!!\n",
      "770 iterations done!!\n",
      "780 iterations done!!\n",
      "790 iterations done!!\n",
      "800 iterations done!!\n",
      "810 iterations done!!\n",
      "820 iterations done!!\n",
      "830 iterations done!!\n",
      "840 iterations done!!\n",
      "850 iterations done!!\n",
      "860 iterations done!!\n",
      "870 iterations done!!\n",
      "880 iterations done!!\n",
      "890 iterations done!!\n",
      "900 iterations done!!\n",
      "910 iterations done!!\n",
      "920 iterations done!!\n",
      "930 iterations done!!\n",
      "940 iterations done!!\n",
      "950 iterations done!!\n",
      "960 iterations done!!\n",
      "970 iterations done!!\n",
      "980 iterations done!!\n",
      "990 iterations done!!\n",
      "Accuracy of test spgd: 8.400000 %\n"
     ]
    }
   ],
   "source": [
    "pgd_correct = 0\n",
    "spgd_correct = 0\n",
    "total = 0\n",
    "\n",
    "pgd = []\n",
    "spgd = []\n",
    "print(f\"Length of training dataloader: {len(train_dataloader)}\")\n",
    "\n",
    "for i, (images, labels) in enumerate(train_dataloader):\n",
    "    labels = labels.to(args.device)\n",
    "    images = images.to(args.device)\n",
    "\n",
    "    # pgd_adv_images = pgd_attack(classifier_model, images, labels)\n",
    "    spgd_adv_images = semantic_pgd_attack(classifier_model, autoencoder_model, images, labels)\n",
    "    # print(type(pgd_adv_images), type(spgd_adv_images))\n",
    "    # print(pgd_adv_images.shape, spgd_adv_images.shape)\n",
    "    # pgd.append(pgd_adv_images)\n",
    "    spgd.append(spgd_adv_images)\n",
    "\n",
    "    # pgd_outputs = classifier_model(pgd_adv_images)\n",
    "    spgd_outputs = classifier_model(spgd_adv_images)\n",
    "    \n",
    "    # _, pgd_pre = torch.max(pgd_outputs.data, 1)\n",
    "    _, spgd_pre = torch.max(spgd_outputs.data, 1)\n",
    "\n",
    "    total += 1\n",
    "    # pgd_correct += (pgd_pre == labels).sum()\n",
    "    spgd_correct += (spgd_pre == labels).sum()\n",
    "    if i == 1000:\n",
    "        break\n",
    "    if i % 10 == 0:\n",
    "        print(f\"{i} iterations done!!\")\n",
    "    \n",
    "    # plt.imshow(torchvision.utils.make_grid(images.cpu().data, normalize=True), [normal_data.classes[i] for i in pre])\n",
    "\n",
    "# pgd = torch.stack(pgd).reshape((-1, 3, 32, 32))\n",
    "spgd = torch.stack(spgd).reshape((-1, 3, 32, 32))\n",
    "# print(pgd.shape, spgd.shape)\n",
    "\n",
    "# print('Accuracy of test pgd : %f %%' % (100 * float(pgd_correct) / (total)))\n",
    "print('Accuracy of test spgd: %f %%' % (100 * float(spgd_correct) / (total)))\n",
    "\n",
    "# plot_adversarial_images(images, pgd_adv_images, spgd_adv_images, reshape_size=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "def plot_images(images):\n",
    "    plt.figure(figsize=(20, 2))\n",
    "    images = torch.Tensor(images).reshape(-1, 3, 32, 32)\n",
    "    grid = torchvision.utils.make_grid(images, nrow=1, normalize=True, range=(-1,1))\n",
    "    grid = grid.permute(1, 2, 0)\n",
    "    plt.imshow(grid)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "images = images.cpu().detach()\n",
    "# pgd_adv_images = pgd_adv_images.cpu().detach()\n",
    "spgd_adv_images = spgd_adv_images.cpu().detach()\n",
    "\n",
    "# plot_images(torch.stack([images, pgd_adv_images, spgd_adv_images]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LPIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsh/anaconda3/envs/py38/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/harsh/anaconda3/envs/py38/lib/python3.8/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:53] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average LPIPS score of modifed adversarial attack:  tensor(0.2368, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import lpips\n",
    "loss_fn_alex = lpips.LPIPS(net='alex') # best forward scores\n",
    "\n",
    "# LPIPS between original and original attacks\n",
    "import torch\n",
    "# img_orig = torch.Tensor(pgd_adv_images) # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "img_modf = torch.Tensor(spgd_adv_images)\n",
    "img = images.detach().cpu()\n",
    "\n",
    "# orig_lpips = loss_fn_alex(img, img_orig)\n",
    "modf_lpips = loss_fn_alex(img, img_modf)\n",
    "# print(\"Average LPIPS score of original adversarial attack: \", orig_lpips.flatten().mean())\n",
    "print(\"Average LPIPS score of modifed adversarial attack: \", modf_lpips.flatten().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n",
    "\n",
    "def lpips_pgd_attack(model, ae_model, images, labels, eps=0.4, alpha=8/255, s_alpha=0.007, iters=100, batch_size=1) :\n",
    "    lpips = LearnedPerceptualImagePatchSimilarity(net_type='vgg')\n",
    "    images = images.to(args.device)\n",
    "    labels = labels.to(args.device)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "    adv_images = images.clone().detach()\n",
    "    original_inputs_numpy = images.clone().cpu().detach().numpy()\n",
    "        \n",
    "    for i in range(iters) :\n",
    "        adv_images.requires_grad = True\n",
    "        # adv_images = adv_images.to(args.device)\n",
    "        \n",
    "        # Classifier part\n",
    "        outputs = model(adv_images)\n",
    "\n",
    "        model.zero_grad()\n",
    "        cost = loss(outputs, labels).to(args.device)\n",
    "        cost.backward()\n",
    "\n",
    "        # # Perturb original image\n",
    "        # adv_images = images + alpha*images.grad.sign()\n",
    "\n",
    "        # # Adjust semantics\n",
    "        # adv_images = adv_images.detach_()\n",
    "        # adv_images.requires_grad = True\n",
    "        # recon_images, _ = ae_model(adv_images)\n",
    "        # ae_model.zero_grad()\n",
    "\n",
    "        # Calculate grad w.r.t classifier\n",
    "        grad_classifier = adv_images.grad.cpu().detach()\n",
    "\n",
    "        # LPIPS part\n",
    "        adv_images.grad = None\n",
    "        lpips_loss = lpips(images, adv_images)\n",
    "        # recon_images, _ = ae_model(adv_images)\n",
    "        # ae_model.zero_grad()\n",
    "\n",
    "        # mse_loss = F.mse_loss(adv_images, recon_images).to(args.device)\n",
    "        lpips_loss.backward()\n",
    "        # mse_loss.backward()\n",
    "\n",
    "        # Calculate grad w.r.t autoencoder\n",
    "        grad_autoencoder = adv_images.grad.cpu().detach()\n",
    "\n",
    "        # Check if the attack is successful\n",
    "        has_attack_succeeded = (outputs.cpu().detach().numpy().argmax(1)!=labels.cpu().numpy())\n",
    "\n",
    "        # Calculate the projection of perceptual grad on classifier\n",
    "        # and grad_autoencoder = grad_autoencoder - (projection of gradient_autoencoder onto gradient_classifier)\n",
    "        grad_autoencoder_proj = grad_autoencoder - torch.bmm((torch.bmm(grad_autoencoder.view(batch_size, 1, -1), \n",
    "                                grad_classifier.view(batch_size, -1, 1)))/(1e-20+torch.bmm(grad_classifier.view(batch_size, 1, -1),\n",
    "                                grad_classifier.view(batch_size, -1, 1))).view(-1, 1, 1),\n",
    "                                grad_classifier.view(batch_size, 1, -1)).view(grad_autoencoder.shape)\n",
    "\n",
    "        # Calculate the projection of classifier grad on autoencoder\n",
    "        # and grad_classifier = grad_classifier - (projection of gradient_classifier onto gradient_autoencoder)\n",
    "        grad_classifier_proj = grad_classifier - torch.bmm((torch.bmm(grad_classifier.view(batch_size, 1, -1),\n",
    "                               grad_autoencoder.view(batch_size, -1, 1)))/(1e-20+torch.bmm(grad_autoencoder.view(batch_size, 1, -1),\n",
    "                               grad_autoencoder.view(batch_size, -1, 1))).view(-1, 1, 1),\n",
    "                               grad_autoencoder.view(batch_size, 1, -1)).view(grad_classifier.shape)\n",
    "\n",
    "        # Combine grads (Selective Gradient Descent)\n",
    "        # grad = grad_classifier * (1-has_attack_succeeded) - grad_autoencoder * has_attack_succeeded\n",
    "        grad = grad_classifier_proj * (1-has_attack_succeeded) - (grad_autoencoder_proj) * has_attack_succeeded\n",
    "\n",
    "        # Add semantic perturbation\n",
    "        # adv_examples = adv_images - s_alpha * sign_data_grad\n",
    "\n",
    "        # Apply combined perturbation\n",
    "        sign_grad = torch.sign(grad).to(torch.float32).to(args.device)\n",
    "        adv_images = adv_images.detach() + alpha * sign_grad\n",
    "\n",
    "        eta = torch.clamp(adv_images - torch.tensor(original_inputs_numpy).to(args.device), min=-eps, max=eps)\n",
    "        adv_images = torch.clamp(torch.tensor(original_inputs_numpy).to(args.device) + eta, min=0, max=1).detach()\n",
    "            \n",
    "    return adv_images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00aac633587f1360e1f47c4e664177479df7da68016053e1dd2bae3421e00d8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
